---
title: "notes"
output:
  md_document:
    variant: markdown_github
    toc: TRUE
    toc_depth: 3
---


## Some notes on retrieval augmented generation (RAG) in R

```{r include=FALSE}
Sys.setenv(OPENAI_API_KEY = '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')
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("/home/jtimm/pCloudDrive/GitHub/git-projects/render_toc.r")
```



```{r echo=FALSE}
render_toc("/home/jtimm/pCloudDrive/GitHub/git-projects/rag-notes/README.Rmd", 
           toc_header_name = 'RAG Notes',
           toc_depth = 2)
```


## Query expansion

```{r message=FALSE, warning=FALSE}
library(dplyr)
query99 <- 'Why is Generation X wary of President Biden \n
            as the 2024 presidential election looms? \n'

system99 <- 'You are a political analyst.'
```


> A simple utility function for chat completion via Open AI.

```{r}
get_chat_response <- function(prompt, system){
  
  resp <- openai::create_chat_completion(
      model = "gpt-4",
      messages = list(list("role" = "system", 
                           "content" = system),
                      list("role" = "assistant", 
                           "content" = prompt)) 
      )
  
  # Extract generated text
  resp0 <- resp$choices$message.content
  # Re-format/clean
  resp1 <- Filter(function(x) x != "", strsplit(resp0, '\n')[[1]])
  data.frame(resp = resp1)
}
```



### Augmented query

> These generated queries are designed to explore different facets or interpretations of the original query, thereby broadening the scope of the search.

1. Use LLM to generate similar queries to original user query
2. Embed original and augmented queries
3. Retrieve documents (via semantic search) using both the original and augmented query vectors
4. Consolidate highest ranked retrieved documents as relevant contexts for LLM


```{r}
p1 <- 'For the question above, suggest up to five additional related questions.\n
       Keep suggested questions short.\n
       Suggest a variety of questions that cover different aspects of the topic.\n 
       Output one question per line. \n
       Do not number the questions.'

get_chat_response(prompt = paste0(query99 |> toupper(), p1),
                  system = system99) |> 
  knitr::kable()
```




### "Step-back" prompting

1. Use LLM to provide a more general user query
2. Embed original and more general queries
3. Retrieve documents using both the original and more general vectors
4. Aggregate highest ranked retrieved documents


```{r}
p2 <- 'Provide five *more general* versions of the question above. \n
       Output one question per line. Do not number the questions.'

get_chat_response(prompt = paste0(query99 |> toupper(), p2),
                  system = system99) |> 
  knitr::kable()
```



### Hypothetical answer (HyDE)

1. Use LLM to provide a hypothetical answer to user query
2. Embed user query & hypothetical answer
3. Retrieve documents using both query & hypothetical answer (or a concatenation of the two) vectors
4. Aggregate highest ranked retrieved documents

```{r}
hyde <- 'Provide an example answer to the question above, limited to 50 words: \n\n'

get_chat_response(prompt = paste0(query99 |> toupper(), hyde), 
                  system = system99) |> 
  knitr::kable()
```



### Hypothetical questions

1. Use LLM to generate specific questions for each text chunk
2. Embed chunk questions
3. Retrieve documents via question vectors
4. Aggregate highest ranked retrieved documents


```{r}
library(dplyr)
articles <- textpress::web_scrape_urls(x = 'Generation X', cores = 6)

chunks <- articles |>
  filter(!is.na(text)) |>
  mutate(doc_id = row_number()) |>
  
  textpress::nlp_split_sentences() |>
  textpress::rag_chunk_sentences(chunk_size = 2, 
                                 context_size = 1)
```




```{r}
set.seed(99)
chunks_sample <- chunks |>
  filter(grepl('Generation X|Gen X', chunk_plus_context)) |>
  sample_n(3)

answers <- paste0(chunks_sample$chunk_plus_context,  collapse = '\n\n')

question <- 'For each of the contexts provided below, \n
             generate a hypothetical question for which \n
             the context could serve as an answer.\n
             Output one question per line. \n
             Keep questions simple. Avoid "compound" questions. \n
             Do not number the questions.'

hyp_question <- get_chat_response(prompt = paste0(question, answers),
                                  system = '')

hyp_question[1:3,] |> 
  cbind(chunks_sample$chunk_plus_context) |>
  knitr::kable()
```



## Building contexts

### Sentence Window Retrieval



## Evaluation




